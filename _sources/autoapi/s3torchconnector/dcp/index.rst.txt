s3torchconnector.dcp
====================

.. py:module:: s3torchconnector.dcp


Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/s3torchconnector/dcp/s3_file_system/index
   /autoapi/s3torchconnector/dcp/s3_prefix_strategy/index


Classes
-------

.. autoapisummary::

   s3torchconnector.dcp.S3FileSystem
   s3torchconnector.dcp.S3StorageReader
   s3torchconnector.dcp.S3StorageWriter
   s3torchconnector.dcp.S3PrefixStrategyBase
   s3torchconnector.dcp.DefaultPrefixStrategy
   s3torchconnector.dcp.NumericPrefixStrategy
   s3torchconnector.dcp.BinaryPrefixStrategy
   s3torchconnector.dcp.HexPrefixStrategy


Package Contents
----------------

.. py:class:: S3FileSystem(region: str, s3_client: Optional[s3torchconnector._s3client.S3Client] = None, s3client_config: Optional[s3torchconnector.S3ClientConfig] = None, reader_constructor: Optional[s3torchconnector.s3reader.S3ReaderConstructorProtocol] = None)

   Bases: :py:obj:`torch.distributed.checkpoint.filesystem.FileSystemBase`


   Helper class that provides a standard way to create an ABC using
   inheritance.


   .. py:method:: create_stream(path: Union[str, os.PathLike], mode: str) -> Generator[io.IOBase, None, None]

      Create a stream for reading or writing to S3.

      :param path: The S3 path to read or write.
      :type path: Union[str, os.PathLike]
      :param mode: The mode for the stream. Supports 'rb' for read mode and 'wb' for write mode.
      :type mode: str

      :Yields: *io.BufferedIOBase* -- A stream for reading or writing to S3.

      :raises ValueError: If the mode is not 'rb' or 'wb'.



   .. py:method:: concat_path(path: Union[str, os.PathLike], suffix: str) -> str

      Concatenate a suffix to the given path.

      :param path: The base path.
      :type path: Union[str, os.PathLike]
      :param suffix: The suffix to concatenate.
      :type suffix: str

      :returns: The concatenated path.
      :rtype: str



   .. py:method:: init_path(path: Union[str, os.PathLike]) -> Union[str, os.PathLike]

      Initialize the path for the filesystem.

      :param path: The path to initialize.
      :type path: Union[str, os.PathLike]

      :returns: The initialized path.
      :rtype: Union[str, os.PathLike]



   .. py:method:: rename(old_path: Union[str, os.PathLike], new_path: Union[str, os.PathLike]) -> None

      Rename an object in S3.

      This is emulated by copying it to a new path and deleting the old path. The deletion part is retried (see also
      :func:`S3FileSystem._delete_with_retry`).

      :param old_path: The current path of the object.
      :type old_path: Union[str, os.PathLike]
      :param new_path: The new path for the object.
      :type new_path: Union[str, os.PathLike]

      :raises ValueError: If the old and new paths point to different buckets.
      :raises S3Exception: If there is an error with the S3 client.



   .. py:method:: mkdir(path: Union[str, os.PathLike]) -> None

      No-op method for creating directories in S3 (not needed).



   .. py:method:: exists(path: Union[str, os.PathLike]) -> bool


   .. py:method:: rm_file(path: Union[str, os.PathLike]) -> None


   .. py:method:: validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) -> bool
      :classmethod:



.. py:class:: S3StorageReader(region: str, path: Union[str, os.PathLike], s3client_config: Optional[s3torchconnector.S3ClientConfig] = None, reader_constructor: Optional[s3torchconnector.s3reader.S3ReaderConstructorProtocol] = None)

   Bases: :py:obj:`torch.distributed.checkpoint.filesystem.FileSystemReader`


   Interface used by ``load_state_dict`` to read from storage.

   One StorageReader instance acts as both the coordinator and the follower
   in a distributed checkpoint. As part of initialization, each instance
   is told its role.

   A subclass should expected the following sequence of calls by ``load_state_dict``:

   0) (all ranks) set checkpoint_id if users pass a valid checkpoint_id.
   1) (all ranks) read_metadata()
   2) (all ranks) set_up_storage_reader()
   3) (all ranks) prepare_local_plan()
   4) (coordinator) prepare_global_plan()
   5) (all ranks) read_data()


   .. py:attribute:: fs


   .. py:attribute:: path
      :value: ''



   .. py:attribute:: sync_files
      :value: False



   .. py:method:: validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) -> bool
      :classmethod:


      Check if the given checkpoint_id is supported by the storage. This allow
      us to enable automatic storage selection.



   .. py:method:: prepare_local_plan(plan: torch.distributed.checkpoint.planner.LoadPlan) -> torch.distributed.checkpoint.planner.LoadPlan

      Sort load items by storage offset for sequential access optimization.

      :param plan: The load plan from PyTorch DCP.
      :type plan: LoadPlan

      :returns: The same plan with items sorted by storage offset.
      :rtype: LoadPlan



.. py:class:: S3StorageWriter(region: str, path: str, s3client_config: Optional[s3torchconnector.S3ClientConfig] = None, prefix_strategy: Optional[s3torchconnector.dcp.s3_prefix_strategy.S3PrefixStrategyBase] = None, thread_count: int = 1, **kwargs)

   Bases: :py:obj:`torch.distributed.checkpoint.filesystem.FileSystemWriter`


   Basic implementation of StorageWriter using file IO.

   This implementation makes the following assumptions and simplifications:

   * The checkpoint path is an empty or non-existing directory.
   * File creation is atomic

   The checkpoint consist of one file per write request plus
   a `.metadata` file with the serialized metadata.



   .. py:attribute:: fs


   .. py:attribute:: path
      :value: ''



   .. py:attribute:: prefix_strategy


   .. py:method:: prepare_global_plan(plans: List[torch.distributed.checkpoint.planner.SavePlan]) -> List[torch.distributed.checkpoint.planner.SavePlan]

      Prepare save plans with S3-specific storage metadata.

      :param plans: List of save plans to be processed.

      :returns: Modified save plans with S3 storage metadata.



   .. py:method:: validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) -> bool
      :classmethod:


      Check if the given checkpoint_id is supported by the storage. This allow
      us to enable automatic storage selection.



.. py:class:: S3PrefixStrategyBase

   Bases: :py:obj:`abc.ABC`


   Base class for S3 prefix generation strategies.


   .. py:method:: generate_prefix(rank: int) -> str
      :abstractmethod:


      Generate storage prefix for the given rank.



.. py:class:: DefaultPrefixStrategy

   Bases: :py:obj:`S3PrefixStrategyBase`


   Default strategy for generating S3 prefixes.


   .. py:method:: generate_prefix(rank: int) -> str

      Generate simple rank-based name without prefix.



.. py:class:: NumericPrefixStrategy(base: int, epoch_num: Optional[int] = None, min_prefix_length: int = 10, prefix_count: Optional[int] = None)

   Bases: :py:obj:`S3PrefixStrategyBase`


   Base class for numeric prefix generation strategies.


   .. py:attribute:: base


   .. py:attribute:: epoch_num
      :value: None



   .. py:attribute:: min_prefix_len
      :value: 10



   .. py:attribute:: prefix_count
      :value: 1



   .. py:attribute:: prefix_map


   .. py:method:: generate_prefix(rank: int) -> str

      Generate numeric-based prefix with optional epoch number.

      :param rank: Process rank in the distributed environment.

      :returns: <pattern>/epoch_<num>/__<rank>_
                or <pattern>/__<rank>_ if no epoch number is provided.
      :rtype: Prefix string in format



.. py:class:: BinaryPrefixStrategy(epoch_num: Optional[int] = None, min_prefix_length: int = 10, prefix_count: Optional[int] = None)

   Bases: :py:obj:`NumericPrefixStrategy`


   Binary (Base2) prefix generation strategy using only 0 and 1.


.. py:class:: HexPrefixStrategy(epoch_num: Optional[int] = None, min_prefix_length: int = 10, prefix_count: Optional[int] = None)

   Bases: :py:obj:`NumericPrefixStrategy`


   Hexadecimal-based prefix generation strategy.


